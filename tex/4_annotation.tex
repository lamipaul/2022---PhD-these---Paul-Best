\chapter{Optimising annotation processes}
\label{chap:annotation}
\minitoc

\section{Context and objective}

Given the large amount of available recordings presented in the previous section, the objective of this thesis is to build robust detection and classification mechanisms for the vocalisations of species of interest. For this purpose and with the chosen approach of \acp{ANN}, annotated databases are needed. In the following chapter, procedures and \acp{UI} suited for bioacoustic use cases are proposed, with an objective of optimising annotation quantity while minimising human effort. For all tasks, the annotation procedure can be summarised in 5 steps that are illustrated in Figure \ref{fig:flowchart}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/annot_flowchart.pdf}
    \caption{Flow chart of procedures employed in the annotation processes.}
    \label{fig:flowchart}
\end{figure}

This chapter starts by introducing a versatile and efficient approach to annotation (thumbnail picking), it will be used in the subsequent experiments. Then, algorithms and \acp{UI} are proposed for several use cases, each being adapted to specific constraints:
\begin{itemize}\setlength{\itemsep}{1pt}
    \item To detect stationary signals with some samples to tune a handcrafted algorithm, a spectrogram binarisation approach is described (applied to orca calls of OrcaLab in section \ref{chap:spec_bin}).
    \item Looking for transitory signals in stereophonic recordings, I propose an interface to visualise and annotate \acp{TDOA} tracks (applied to sperm whale clicks of Bombyx in section \ref{chap:bombyx_annot}).
    \item For a case when no target signals are available a priori, a generic extraction of spectral distributions is used to cluster similar acoustic events (applied to humpback whale calls of Carimam in section \ref{chap:HB_annot}).
    \item In contrast, when a large quantity of signals of interest are available, an \ac{AE} demonstrated the ability to learn relevant features to cluster calls with similar frequency contours (applied to orca call types of OrcaLab in section \ref{chap:autoencoder}).
\end{itemize}
%
Finally, after employing these methods to gather an initial set of annotations, active learning was conducted until a satisfying amount of labels were available (section \ref{chap:active}). The resulting datasets are presented with their chosen train / test split in the last section of this chapter.


\section{Thumbnail picking} \label{chap:png_annot}

Often during annotation procedures, we want to manually sort out true and false positives from a set of detections. It occurred numerous times during this thesis, after preliminary detection algorithms or during the active learning process. Picking spectrogram images from their thumbnails in file explorers appeared to be the most efficient way to do it (see Fig.~\ref{fig:png_annot}).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/thunar.png}
    \caption{Example of thumbnails ready to be annotated (picked), using the Thunar file explorer \cite{thunar}. Here, files are clustered spectrograms of orca calls (see section~\ref{chap:autoencoder}).}
    \label{fig:png_annot}
\end{figure}

The typical scenario in which this procedure was used is to pick false positives from a set of detections. In a few minutes, an annotator can browse hundreds of samples (exhaustively or not), and select dozens of files to move them to a new folder. Having identifiers in filenames then allows to retrieve the annotator's decisions and update a dataset accordingly.

Annotating by organising thumbnails in folders is not only efficient in time, but also very versatile (it requires no specific software installation). This comes practical especially when needing annotation efforts from different experts with different operating systems for example.


\section{Gathering regions of interest}

To get an initial set of events to annotate, we can use our prior knowledge of target signals and run an algorithm that filters the data in that regard. These handcrafted algorithms present limitations (argued in section~\ref{chap:limitations}), but avoid having to go through long chunks of void in recordings to find our first training examples.

In detection algorithms, the user usually sets a threshold to binarise continuous prediction values. In the case of cetacean vocalisations, the threshold is typically on the energy level at a specific frequency, or on the cross-correlation coefficient (template matching approaches). The lower we set this threshold, the lower the specificity (higher risk of false detections) but also the higher the sensitivity (lower risk of missed detections). Conversely, by increasing this threshold, we increase the specificity but decrease the sensitivity.

This trade-off is to be kept in mind when tuning handcrafted algorithms to build a first database: we want just enough sensitivity to yield some true positives (perhaps the ones with the highest \ac{SNR}), while keeping the number of detections low enough so that we can go through them in a reasonable amount of time. 

The following paragraphs introduce two case studies of such approaches, one with stationary signals (orca calls) and one with transitory ones (sperm whale clicks).


\subsection{Spectrogram energy thresholding (orca calls)}\label{chap:spec_bin}
\textit{This work was conducted in collaboration with Jan Schlüter and Marion Poupard, on the OrcaLab data introduced in section \ref{chap:data_Toulon}).}\\
The chosen approach to the preliminary detection of orca calls was inspired by \citet{lasseck2014large} on spectrogram segmentation for bird call detection. We first binarise spectrograms with adaptive thresholds that use rows and columns moments (Fig.~\ref{fig:fig_binarisation}). The original formula proposed by \citet{lasseck2014large} for the threshold $\mathrm{T}_{f,t}$ given a log compressed spectrogram $\mathbf{E}$ is given in Eq.~\ref{lasseck}. The goal being to detect pixels with energy values above the distribution of their row and column, we propose to rather use Eq.~\ref{binarise}.

\begin{equation}\label{lasseck}
    \mathrm{T}_{f,t} = \max(3 \times \underset{j}{\median}(\mathrm{E}_{f,j}), \; 3 \times \underset{i}{\median}(\mathrm{E}_{f,i})).
\end{equation}
\begin{equation}\label{binarise}
    \mathrm{T}_{f,t} = \max( \underset{j}{\median}(\mathrm{E}_{f,j}) + 2 \times \underset{j}{\std}(\mathbf{E}_{f,j}), \; \underset{i}{\median}(\mathbf{E}_{i,t}) + \underset{i}{\std}(\mathbf{E}_{i,t})).
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/lasseck.pdf}
    \caption{Comparison of the spectrogram binarisation procedure following Eq.~\ref{lasseck} (middle) and Eq.~\ref{binarise} (right).}
    \label{fig:fig_binarisation}
\end{figure}

Connected positive pixels are later grouped by regions, from which we will extract features such as minimum and maximum frequencies, duration, and mean and maximum decibels. We finally use our prior knowledge of orca calls to filter out impossible regions (features that are out of range), and plot the remaining ones for annotation via thumbnail picking. With such algorithms, the user can filter detections to get a first set of positive annotations, but also has an opportunity to fetch negative ones (strong acoustic events that might confuse the model later on).


\subsection{\acs{TDOA} tracking (sperm whale clicks)}\label{chap:bombyx_annot}
\textit{This work was conducted in collaboration with Maxence Ferrari and Marion Poupard, on the Bombyx data introduced in section \ref{chap:data_Toulon}.}\\
For sperm whale clicks, time domain signal processing is more appropriate than the spectral based energy detection presented above. We thus propose the following procedure. In a first pre-processing step, sperm-whale clicks are emphasised by correlating the signal with a sinus of their centroid frequency (12.5\,kHz). Then, the permissive detection mechanism is based on thresholding on the \ac{TK} energy operator (inspired by \citet{kandia2006detection}). Finally, for all detected impulses, \acp{TDOA} are computed between the two hydrophones of the antenna. Among the large amount of false positives, clusters of stable \ac{TDOA} appear, revealing localised acoustic sources which are here considered as signals of interest.

In our data the three main signals that trigger such a detector are those produced by sperm whales, boats, and other odontoceti such as long-finned pilot whales (\textit{Globicephala melas}). To browse large amount of recordings and efficiently discriminate between the three for annotation, the custom \ac{UI} shown in Figure~\ref{fig:cacha_annot_UI} was built.

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{fig/interace_bombyx.png}
    \caption{Custom \ac{UI} built in matplotlib \cite{matplotlib} for the annotation of sperm whale clicks. (top) \acp{TDOA} versus time of detected clicks, with vertical bars denoting gaps between recorded files. (bottom) Spectrogram of the signal surrounding the selected click, shown with a red dot on the top panel.}
    \label{fig:cacha_annot_UI}
\end{figure}

This \ac{UI} shows the scatter plot of preliminary detections (time versus \acp{TDOA}). It enables to identify tracks of moving acoustic sources, with the slope reflecting the angular speed relative to the antenna. With such a plot, we display 10 hours of signal at once. When a \ac{TDOA} track is identified, the user can click on it. The associated detection will be signaled with a red dot, its surrounding spectrogram displayed and the sound played. This allows for the identification of the source responsible for the selected track, which can be saved by clicking on the associated button (noise, pilot whale or sperm whale).


\section{Feature extraction and filtering}

Clustering allows for a strong optimisation of the annotation process. Indeed, once signals are grouped by similarity, browsing and sorting becomes much more efficient, especially by avoiding to go through large amounts of void.

The key to clustering quality is the extraction of relevant features for similarity measurement. Hereby are presented three feature extraction approaches on different kinds of signals : humpback whale vocalisations, toothed whale clicks, and orca calls.

Once features were extracted, they were usually projected using \ac{UMAP} \cite{mcinnes2018umap} before a \ac{DBSCAN} clustering \cite{ester1996density}. \citet{allaoui2020considerably} have shown that dimensionality reduction using \ac{UMAP} would improve the performance of clustering such as density based ones. The distribution of projections in turn motivated a density based approach to clustering such as \ac{DBSCAN} (Fig.~\ref{fig:cluster_interface_HB}).


\subsection{Spectro-temporal features (humpback whale calls)} \label{chap:HB_annot}
\textit{This work was conducted on the Carimam dataset described in section \ref{chap:data_Toulon}.}\\
The objective of the following procedure is to explore a large dataset with no samples of target signals given a priori. For this purpose, a relatively generic feature extraction was conducted before plotting and clustering their projection. Like so, we intend to isolate groups of similar events, and allow for a more efficient exploration of the data. Especially, the events we hope to find are click trains and cetacean vocalisations, but we also expect to retrieve events from other noise sources.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/HB_specextract.pdf}
    \caption{Main steps of the spectral feature extraction procedure. The spectrogram is first max-pooled time-wise by a given factor. Then, each frequency bin is sorted (time-wise) to make abstraction of the temporal position of events.}
    \label{fig:feat_spec}
\end{figure}

Spectrogram chunks are processed in two main steps in order to emphasis potential signals of interests (Fig.~\ref{fig:feat_spec}). First, to get a representation that preserves short events (such as clicks) but with a reduced size, we max-pool spectrograms time wise. Second, to make abstraction of the temporal information (whether an event is at the beginning of a chunk or at its end) we sort each frequency bin (time wise) in descending order.

Doing so, the resulting matrix is no longer a spectrogram, but rather a representation of the energy distribution for each frequency bin. This allows to select specific columns as discriminating features, the first denoting the highest energy in the chunk for each frequency bin, and the last their lowest. For instance, looking for short events, we can select the first and second columns. Chunks with a large gap between the two should contain a short but strong acoustic event. It could thus be differentiated from chunks with stationary strong energy and chunks with a low overall energy, and this for specific frequency bins. For simplicity, this set of columns to be kept will be referred to as `quantiles'.
The full procedure for this analysis is described in Listing~\ref{hb_cluster}.

\begin{python}[caption={Feature extraction and clustering for humpback whale vocalisations. Steps are operated over a batch of signals on \ac{GPU} for computation speed.}, label={hb_cluster}]
from torchaudio.functional import resample
import torch
from sklearn.cluster import DBSCAN
from umap import UMAP
gpu = torch.device('cuda')

# load a batch of signals using pyTorch DataLoader
sigs = ... 
sigs = sigs.to(gpu)
sigs = resample(sigs, source_fs, fmax * 2)
# compute the magnitude spectrogram using the STFT
specs = torch.stft(sigs, n_fft=1024, hop_length=512)
specs = 20 * torch.log10(specs.norm(p=2, dim=-1))
# substract a background noise estimate
specs = specs - specs.median(dim=1, keepdim=True)[0]
# apply the mel-transform
spec = torch.matmul(melbank, specs)
# undersample the spectrogram over the time dimension
specs = torch.nn.MaxPool1d((uds,))(specs)
# rearange the tensor into a list of time chunks
specs = specs.permute(1, 0, 2)
specs = specs.reshape(specs.shape[0], -1, chunksize)
specs = specs.permute(1, 0, 2)
# sort frequency bins and select quantiles
features = torch.sort(specs, dim=2, descending=True)[0]
features = features[:,:,quantiles].numpy()
# project and cluster each time chunk
features = features.reshape((specs.shape[0], -1))
embeddings = UMAP().fit_transform(features)
clusters = DBSCAN().fit_predict(embeddings)
\end{python}

The variables \pythoninline{fmax}, \pythoninline{uds}, and \pythoninline{chunksize} need to be tuned to the type of signals we desire to isolate, especially in terms of spectrum range and spectrogram temporal resolution. \pythoninline{fmax} determines $f_s$ at which the signal is resampled, \pythoninline{uds} determines the downsampling factor used for max-pooling the spectrogram, and \pythoninline{chunksize} determines the sampling rate of the feature extraction process. As for the humpback whales, they were set to 8,000\,Hz, 14 and 20 respectively (chunks of 10\,sec with 20 time bins). Then, the first second and fifth quantiles were chosen.

These choices were made via intuition and empiric testing. However, once annotations were gathered, experiments were carried out to measure which configuration would have been the most efficient.

Trials with varying values for the size of chunks and the choice of quantiles were conducted, using the \ac{NMI} between clusters and annotations as a metric of configuration quality. The choice of configuration appeared to have a relatively small impact on the resulting \ac{NMI}, with values ranging between 0.15 and 0.18 (the random baseline being under 0.01). The highest scoring configuration was to cut chunks of size 10 keeping only the first quantile.

Once features have been extracted for a large amount of samples, we reduce their dimensionality using \ac{UMAP}, and cluster them using \ac{DBSCAN}. A custom made interface then enables a seamless browse through clustered projections (Fig.~\ref{fig:cluster_interface_HB}). 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/cluster_vocs_interface.png}
    \caption{Interface for browsing projections of spectrogram chunks. The left panel displays the projections, with colours denoting associated clusters and red dots signaling points that have been clicked on. The right panel displays the spectrogram of the last selected audio as well as its metadata.}
    \label{fig:cluster_interface_HB}
\end{figure}

Users can select an audio chunk by clicking on its projection on the scatter plot. The interface will then play the corresponding sound extract and display its spectrogram on a secondary window. This allows for the identification of discriminant clusters to be retained (containing only vocalisations, or only noise for instance). Eventually, we can plot samples belonging to selected clusters as .png files and use thumbnail picking to sort out misclassified samples.


\subsection{Impulses' features (toothed whale clicks)}
\textit{This work was strongly inspired by \citet{frasier2021machine}, conducted in collaboration with Maxence Ferrari and Marion Poupard, on the Carimam data (section~\ref{chap:data_Toulon}).}\\
Besides enhancing the annotation process, clustering can also allow to extract higher level features of signals. For instance, we can cluster clicks by their spectral features and infer \ac{ICI} characteristics, which help discriminate toothed whales click trains from reef noise\footnote{A click train is a sequence of clicks with, to some extent, a locally stable \ac{ICI}.}. This \ac{ICI} measurement would not be possible without the prior feature extraction and clustering because of the numerous false positives present.

Similarly than with sperm whale clicks (section \ref{chap:bombyx_annot}), we start with a generic impulse detection on the waveform before extracting features, this time spectral characteristics rather than \acp{TDOA}. I thus propose the following steps :
%
\begin{itemize}  \setlength{\itemsep}{1pt}
\item Generic impulse detection:
    \begin{itemize}
    \item high pass the signal $x(t)$ at 5\,kHz,
    \item compute the Hilbert transform $H(t)$ of $x(t)$,
    \item compute a running average $a(t)$ to smooth $H(t)$,
    \item convert $a(t)$ into decibels with $20 \times \log_{10}(a(t))$,
    \item compute the $\median$ and $\std$ of $a(t)$,
    \item find peaks of $20 \times \log_{10}(x(t))$ that are 3\,dB above the noise level expressed as $\median + 3 \times \std$,
    \item retain peaks with widths between 0.008 and 1.2ms.
    \end{itemize}
\item Feature extraction:
    \begin{itemize}
        \item compute the \ac{FFT} of a 1ms window surrounding the detected impulse,
        \item compute the 3\,dB centroid frequency,
        \item cluster impulses by their centroid frequency (separately per chunk of 1\,min),
        \item compute \acp{ICI} as the time difference between impulses of the cluster,
        \item fit a gaussian \ac{KDE} on the \ac{ICI} distribution of the cluster,
        \item estimate the peak of the \ac{KDE},
        \item for each cluster, save the peak of the \ac{KDE} (most frequent \ac{ICI}), its width (\ac{ICI} variability), and the mean 3\,dB centroid frequency.
    \end{itemize}
\end{itemize}

The user can eventually filter data on the \ac{KDE} peaks height and width depending on the desired specificity (setting a threshold on the height determines the necessary stability of the \ac{ICI}). An interface then displays a scatter plot of \acp{ICI} against centroid frequencies (Fig.~\ref{fig:interface_clicks}). Again, a click on a point triggers a spectrogram display of the corresponding signal, which can be further analysed and eventually saved for annotation.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/annot_click.png}
    \caption{Interface for browsing click clusters. The left panel displays the mean \ac{ICI} against 3\,dB centroid frequency for each cluster of impulses. Red dots signal the selected cluster of clicks. The right panel displays the spectrogram of audio surrounding the selected cluster as well as its meta data.}
    \label{fig:interface_clicks}
\end{figure}

This method enables to explore large banks of recordings in search for click trains, with no prior knowledge on the target signals. Again, a simple interface significantly accelerates the process simply by linking a display of features to associated signals.


\subsection{AE embeddings (orca calls)} \label{chap:autoencoder}
\textit{This work was conducted on the OrcaLab data (section \ref{chap:data_Toulon}), and has been subject to a workshop intervention \cite{bestvihar}.}\\
For this section, we are interested in the classification of pre-detected orca calls (dataset of 114k orca calls detected by a \ac{CNN} presented in section \ref{chap:orca_detec}). \ac{NRKW} call types, as defined by \citet{ford1987catalogue}, are characterised by temporal pitch patterns. First experiments were thus conducted using a pitch based feature extraction to cluster calls (leading to a workshop intervention \cite{poupard2019large}). However, the estimation of the pitch appeared to be quite unreliable in low and medium \ac{SNR} conditions (discussed in section \ref{chap:sota_pitch}). This led to a shift towards a larger scale extraction of shape (as opposed to local pitch estimates).

Auto-encoders (introduced in section \ref{chap:AE}) are trained to compress data in a lower dimensional space (bottleneck) while keeping the ability to reconstruct it. When the bottleneck is small enough, the reconstruction has to rely on learning structures in the data, exploiting them during compression and decompression. This motivated the use of \acp{AE} for the feature extraction of orca calls, expecting the bottleneck to contain the shape of the call in a low dimensional space.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/encoder.pdf}
    \caption{Architecture of the encoder part of the \ac{AE}. (Bottom) shapes of volumes as $(depth \times height \times width )$. (Top) Operations and kernel shapes as $( height \times width )$.}
    \label{fig:encoder}
\end{figure}

The training framework of the \ac{AE}, illustrated in Figure \ref{fig:AE_archi}, was designed as follows (see Fig. \ref{fig:AE_archi}):
\begin{itemize}
    \item Compute the Mel-spectrogram on windows of 2\,sec around detections ($f_s=22050$, $NFFT=1024$, $hop=330$, $\# Melbands=128$, $f_{min}=300$, $f_{max}=11025$),
    \item Run the encoder to compress the 128x128 image to 16 dimensions (Fig.~\ref{fig:encoder}). Each convolution is followed by batch normalisation and leaky rectifier linear units. The resolution is lowered via strides of 2 for each convolution, and a max-pooling layer,
    \item Run the decoder as the mirror of the encoder. The first 16x4x2 volume is created via a linear layer, and each resolution increase consists in upsampling by nearest value followed by two convolution layers of kernels 3x3,
    \item Compute the \ac{VGG} embedding of the input and of the reconstructed images (\ac{VGG}16 pretrained on ImageNet \cite{deng2009imagenet}, extracting activations after the $6^{th}$ convolutionnal layer),
    \item Use the \ac{MSE} between the two \ac{VGG} embeddings as the loss \\ ($\mathcal{L} = \Sigma \lVert \VGG(\mathbf{E}) - \VGG(\mathbf{\hat{E}})\rVert^2$).
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{fig/AE_archi.pdf}
    \caption{Architecture of the training framework for the \ac{AE} of orca calls using a perceptual loss \cite{johnson2016perceptual}.}
    \label{fig:AE_archi}
\end{figure}

The size of the bottleneck was empirically chosen as the minimum that still enables satisfactory reconstructions: 16 channels. To put it in perspective, in a similar \ac{AE} experiment on orca calls, \citet{bergler2019deep} used a bottleneck of 512 channels.

Figure \ref{fig:reconstructions} demonstrates how, in reconstructions, details of some calls are omitted, and background noise becomes patterned. Indeed, due to the limited amount of information that the bottleneck can fit, the decoder is forced to learn common data structures to reconstruct the data. This is actually beneficiary for our end goal of grouping similar shapes together, and it explains why random background noise, transient clicks, and small variations in call shapes are not found in output spectrograms.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/AE_outputs.pdf}
    \caption{Comparison of input and \ac{AE} reconstructed spectrograms.} 
    \label{fig:reconstructions}
\end{figure}

The bottleneck embeddings were later used as features for clustering (\ac{DBSCAN} on \ac{UMAP} projections). This enabled a drastic reduction of the annotation effort by grouping similar calls together. Thumbnail picking (Fig.~\ref{fig:png_annot}) was then conducted to verify clusters and associate them with types as defined by \citet{ford1987catalogue}.


\section{Active learning}\label{chap:active}

Active learning is the process of iteratively training and annotating to improve a database (qualitatively and/or quantitatively). It is relevant when a training database is not sufficient to achieve satisfactory performances (because too few samples are available or only `easy' ones). By correcting the model's predictions at each iteration, we emphasis on difficult examples and guide it towards robustness.

This active learning process was conducted via thumbnail picking to gather annotations for fin whale pulses, dolphin whistles, humpback whale vocalisations, and orca calls. For humpback whales, the initial training database was gathered using clustered spectral features (section \ref{chap:HB_annot}). For orcas, spectrogram binarisations was used (section \ref{chap:spec_bin}). For the remaining two, transfer learning methods were employed.


\subsection{Transfer learning (fin whale pulses, dolphin whistles)}

Pre-training a model on a database before fine tuning on a different one is called transfer learning. Similar approaches were used to kick-start the active learning process on two detection tasks, as described in the following paragraphs.


\subsubsection{Fin whale pulses} \label{chap:fin_activelearning}

To gather a database of fin whale 20\,Hz pulses from the recordings of Bombyx and Boussole, several handcrafted algorithms were first tested (looking for strong energy peaks in realistic time and frequency ranges). Without any exemplary signal to tune them, and with the wide variety of noises present on both banks of recordings, this approach failed to yield any fin whale signals.

We were lucky to eventually get some help from M. Giani Pavan, who shared some of his recordings of Mediterranean fin whale songs containing 100 pulses \cite{pavan2004passive}. Despite the limited amount of samples, training a small neural network on this data allowed to find similar signals on the Bombyx and Boussole datasets (the model architecture is described in section \ref{chap:low_archi}). This demonstrates the capacity of small neural networks to generalise to different recorders even with very few training samples.


\subsubsection{Dolphin whistles}

For this task, as for the fin whale pulses, we used other sources of data available at the lab as a starting point to the active learning process. This time though, the variability of target signals prevented the use of a small model architecture.

Thus, to enforce the generalisation of the model to other recording systems, the available data was augmented with negative samples from the target recording system (Carimam). By mixing annotated foreign inputs with negative samples from Carimam, we teach the model to be robust to common Carimam perturbations (self noise from the sound card, reef noise), while training on positive samples of the target signal. This `mixing' takes form as a simple summation of the waveforms after their standardisation.


\section{Resulting annotations and train / test splits} \label{chap:splits}

The methods proposed in this section yielded enough annotations to train \ac{ANN} models on each detection / classification tasks (Tab.~\ref{tab:recap_annot}).

\begin{table}[ht]
    \centering
    \begin{tabular}{l|c|c|c}
    \textbf{Target signal} & \textbf{Positives} & \textbf{Negatives} & \textbf{Total} \\ \hline
    Sperm whale clicks &  42\% & 58\% & 5,554 \\
    Fin whale 20\,Hz pulses & 14\% & 86\% & 5,790 \\
    Orca calls & 78\% & 22\% & 6,004 \\
    Humpback whale calls & 67\% & 33\% & 2,427 \\
    Dolphin whistles & 12\% & 88\% & 1,595 \\
    \end{tabular}
    \caption{Summary of the annotations gathered on the data at hand for detection task.}
\label{tab:recap_annot}
\end{table}

For the following experiments, performance measurements need to reflect our end goal, namely training robust models. Robustness, can be defined as the capacity to ignore perturbations, some kind of resilience. In our case, perturbations are sound events and background noises, especially those not seen in training. To measure robustness, our test data must therefore contain new acoustic content, somewhat different from training.

The randomly sampled train / test splits often seen in the machine learning community is insufficient in that sense. Indeed, train and test samples will be extracted from the same vocalisation or noise sequences, thus sharing most of their characteristics. On the other hand, choosing a specific source of data, or if not available a distinct time period, should yield novelty in the test set, and give relevant robustness measures for our models.


\subsubsection{Fin whale pulses} \label{chap:rorq_dataset}

The annotated database gathered for fin whale 20\,Hz pulses offers three different data sources (Tab.~\ref{tab:recap_annot_fin}). Thus, in the experiments, three folds were used : each using two sources for training and the remaining one for testing. The Magnaghi data corresponds to the extracts provided by G. Pavan (see section ~\ref{chap:fin_activelearning}).

\begin{table}[ht]
    \centering
    \begin{tabular}{l|c|c|c}
    \textbf{Data Source} & \textbf{Positives} & \textbf{Negatives} & \textbf{Total} \\ \hline
    Magnaghi &  15\% & 85\% & 688 \\
    Boussole & 9\% & 91\% & 4,528 \\
    Bombyx & 49\% & 51\% & 574 \\
    \textbf{Total} & 14\% & 86\% & 5,790 \\
    \end{tabular}
    \caption{Distribution of annotations of 20\,Hz fin whale pulses. Each source of data was used as test set in a 3 fold manner.}
\label{tab:recap_annot_fin}
\end{table}


\subsubsection{Sperm whale clicks} \label{chap:cach_dataset}

The annotated database of sperm whale clicks coming from only one source of data (Bombyx), The year 2017 was chosen for testing and the remaining for training (Tab.~\ref{tab:recap_annot_sperm}). This choice is motivated by the fact that 2015 has too few samples for the test to be relevant, 2016 has a positive / negative distribution too different than the global dataset, and 2018 has the largest amount of samples which is desirable for training. To improve the  annotation comes from separate files.

Experiments showed that the model would tend to lack sensitivity, with the exception of pilot whale samples which would trigger a low specificity. To tackle this issue, and accounting for the imbalance in the data (Tab.~\ref{tab:recap_annot_sperm}), sperm whale and pilot whale samples were over-sampled during training, by a factor 3 and 10 respectively.

\begin{table}[ht]
    \centering
    \begin{tabular}{c|c|c|c|c}
    \textbf{Recording year} & \textbf{Sperm whale} & \textbf{Boat / Noise} & \textbf{Pilot whale} & \textbf{Total} \\ \hline
    2015 &  48\% & 52\% &  & 256 \\
    2016 & 75\% & 23\% & 2\% & 1,383 \\
    2017 & 32\% & 67\% & 1\% & 1,363 \\
    2018 & 28\% & 68\% & 4\% & 2,552 \\
    \textbf{Total} & 42\% & 55\% & 3\% & 5,554
    \end{tabular}
    \caption{Distribution of annotations for the sperm whale click detection task. The year 2017 was used as test set.}
\label{tab:recap_annot_sperm}
\end{table}


\subsubsection{Humpback whale calls}

For the detection of humpback whale calls, the data recorded from Sint Eustatius island was selected as the test set (Tab.~\ref{tab:recap_hump}). This was motivated by the fact that this site has a representative distribution of classes and is neither too small nor too big ($\sim 10\%$).

\begin{table}[ht]
    \centering
    \begin{tabular}{l|c|c|c}
         \textbf{Station} & \textbf{Positives} & \textbf{Negatives} & \textbf{Total} \\ \hline
         Anguilla & 100 & 0 & 100 \\
         Bahamas & 0 & 45 & 45 \\
         Bermude & 276 & 27 & 303 \\
         Guadeloupe & 666 & 26 & 692 \\
         Jamaica & 0 & 11 & 11 \\
         Martinique & 354 & 37 & 391 \\
         Saint Barthélémy & 173 & 0 & 173 \\
         Sint Eustatius & 204 & 103 & 307 \\
         Saint Martin & 163 & 242 & 405 \\
        \textbf{Total} & 67\% & 33\% & 2,427 \\
    \end{tabular}
    \caption{Distribution of humpback whale calls annotations by recording stations (all from Carimam). The Sint Eustatius data source was used as a test set.}
    \label{tab:recap_hump}
\end{table}


\subsubsection{Dolphin whistles}

For the detection of dolphin whistles, the data recorded from Guadeloupe Breach was selected as test set (Tab.~\ref{tab:recap_dolph}), again for a compromise between class balance (similar to that of the global dataset) and sufficient size.

\begin{table}[ht]
    \centering
    \begin{tabular}{l|c|c|c}
         \textbf{Station} & \textbf{Positives} & \textbf{Negatives} & \textbf{Total}  \\ \hline
         Guadeloupe Breach & 36 & 354 & 390 \\
         Gualdeloupe Anse Bertrand & 0 & 49 & 49 \\
         Saint Barthélémy & 0 & 16 & 16 \\
         Sint Eustatius & 37 & 111 & 148 \\
         Saint Martin & 0 & 34 & 34 \\
         Jamaica & 24 & 10 & 34 \\
         Bonaire & 74 & 25 & 99 \\
         Bermude & 25 & 439 & 464 \\
         Bahamas & 0 & 16 & 16 \\
         Anguilla & 0 & 345 & 345 \\
         \textbf{Total} & 12\% & 88\% & 1,595 \\
    \end{tabular}
    \caption{Distribution of dolphin whistle annotations by recording stations (all from Carimam). The Guadeloupe Breach data source was used as test set.}
    \label{tab:recap_dolph}
\end{table}


\subsubsection{Orca call detection} \label{chap:orca_dataset}

A special recording session was run at OrcaLab in 2019 by \citet{poupard2021intra}, for the study of group dynamics via call triangulation. This protocol made use of a different recording system than the original OrcaLab setup. The manual annotations gathered for this study were used in this thesis, bringing an opportunity to measure the impact of a change in recording hardware on detection mechanisms with no additional annotation effort.

Thus, two test sets were used for the orca call detection task, one from the same antenna than in training but from a different year and one from a different antenna (Tab.~\ref{tab:recap_orca}). A preliminary study using this dataset was published in a conference paper \cite{best2020deep}.

\begin{table}[ht]
    \centering
    \begin{tabular}{c|c|c|c|c}
         \textbf{Recorder} & \textbf{Year} & \textbf{Positives} & \textbf{Negatives} & \textbf{Total} \\ \hline
         OrcaLab network & 2015 - 2017 & 846 & 3,777 & 4,623 \\
         OrcaLab network & 2019 & 111 & 177 & 288 \\
         Different antenna \cite{poupard2021intra} & 2019 & 368 & 725 & 1,093 \\
    \end{tabular}
    \caption{Distribution of orca call binary annotations. The data from 2019 (from OrcaLab and different antenna) was used as test set.}
    \label{tab:recap_orca}
\end{table}


\subsubsection{Orca call classification} \label{chap:orca_clf_dataset}

For the orca call classification task, given the diversity of classes and the singular recording source, the train / test split was simply done by sorting by date and choosing a proportion for test and the rest for train. For instance, the first 10\% of each class were taken for test, and the remaining 90\% were used to train the model.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/types.pdf}
    \caption{Examples of each class of orca call types annotated using clusters of \ac{AE} embeddings. The terminology as defined by \citet{ford1987catalogue} has been used by associating calls with their closest class in the catalogue.}
    \label{fig:types}
\end{figure}

\begin{table}[ht]
    \centering
    \begin{tabular}{c|c}
         \textbf{call type} & \textbf{instances} \\ \hline
         N1 & 854 \\
         N2 & 191 \\
         N3 & 192 \\ 
         N4 & 1213 \\
         N5 & 209 \\
         N9 & 609 \\
         N23 & 469 \\
         other & 109 \\
         Noise & 814
    \end{tabular}
    \caption{Distribution of annotations of orca call types \cite{ford1987catalogue}. The `other' class corresponds to infrequent calls that did not have enough occurrences to form an independent class.}
\end{table}

Figure \ref{fig:types} illustrates samples from each class, using the nomenclature defined by \citet{ford1987catalogue}.


\subsubsection{Antarctic blue and fin whale calls}

The Antarctic mysticetes dataset \cite{miller2021open} was not subject to custom annotations, but still demanded some preprocessing to be fed to a model (windowing of samples, extraction of negatives). Table \ref{tab:recap_soos} summarises the resulting distribution of labels for each data source. Samples from Kerguelen 2005 were chosen as a test set, motivated by its specific recording system and location, as well as its sufficient support of all classes (Tab.~\ref{tab:recap_soos}). The remaining date was used for training.

\begin{sidewaystable}
\centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|} \hline
\textbf{Location} & \textbf{Year} & \textbf{Instrument} & \textbf{Bm A} & \textbf{Bm B} & \textbf{Bm Z} & \textbf{Bm D} & \textbf{Bp 20\,Hz} & \textbf{Bp 20+} & \textbf{Bp DS} & \textbf{Negatives} \\ \hline
Balleny Islands & 2015 & PMEL-AUH &  4\% &  1\% &  1\% &  &  7\% &  2\% &  1\% & 10\% \\ \hline 
Elephant Island & 2013 & AURAL &  10\% &  26\% &  6\% &  71\% &  28\% &  24\% &  16\% & 30\% \\ \hline 
Elephant Island & 2014 & AURAL &  28\% &  14\% &  4\% &  7\% &  38\% &  38\% &  64\% & 6\% \\ \hline 
Greenwich 64S & 2015 & Sono.Vault &  3\% &  2\% &  1\% &  &  &  &  1\% & 1\% \\ \hline 
MaudRise & 2014 & AURAL &  9\% &  1\% &  1\% &  &  &  &  & 3\% \\ \hline 
Ross Sea & 2014 & PMEL-AUH &  &  &  &  &  &  &  & 9\% \\ \hline 
Casey & 2014 & AAD-MAR &  15\% &  20\% &  43\% &  4\% &  &  &  & 8\% \\ \hline 
Casey & 2017 & AAD-MAR &  7\% &  8\% &  5\% &  4\% &  1\% &  3\% &  & 8\% \\ \hline 
Kerguelen 1 & 2005 & ARP &  6\% &  3\% &  7\% &  3\% &  6\% &  1\% &  7\% & 9\% \\ \hline 
Kerguelen 2 & 2014 & AAD-MAR &  10\% &  17\% &  22\% &  3\% &  15\% &  24\% &  5\% & 8\% \\ \hline 
Kerguelen 2 & 2015 & AAD-MAR &  8\% &  8\% &  9\% &  8\% &  4\% &  9\% &  5\% & 8\% \\ \hline 
\textbf{Total} & & & 25,177 & 6,903 & 2,515 & 15,339 & 12,933 & 7,761 & 6,381 & 357,765 \\ \hline 
    \end{tabular}
    \caption{Distribution of annotations published by \citet{miller2021open}. The Kerguelen 2005 was chosen as test set.}\label{tab:recap_soos}
\end{sidewaystable}


\section{Discussion}

As seen throughout this chapter, techniques employed in pre-detection, feature extraction and filtering need to be adapted to the type of target signals and the available recordings. For that matter, Table \ref{tab:steps} recapitulates the choices made for each of the 6 annotation procedures conducted.

\begin{table}[ht]
    \centering\resizebox{\linewidth}{!}{    
    \begin{tabular}{c|ccc}
         \textbf{Target signal} & \textbf{Pre-detection} & \textbf{Feature extraction} & \textbf{Filtering} \\ \hline
         Sperm whale clicks & TK filter & \ac{TDOA} & custom \ac{UI} \\ 
         Humpback whale calls & NA & spectral features & custom \ac{UI} and clustering \\
         Orca call detection & spectrogram thresholding & region statistics & hand-crafted rules \\ 
         Orca call classification & \ac{CNN} & auto-encoder & clustering \\ \hline
         fin whale 20\,Hz pulses & \multicolumn{3}{c}{transfer learning} \\
         Dolphin whistles & \multicolumn{3}{c}{transfer learning} \\
    \end{tabular}}
    \caption{Summary of steps employed in the initial annotation process of each target signal.}
    \label{tab:steps}
\end{table}

Often, we need to explore the relevance of extracted features before using them for filtering. This exploration process is significantly accelerated by interactive visualisations that load the signal from a click on projected features. Such exploration helps in finding patterns in the data, and to exploit them to enhance annotations.

The following paragraphs highlight the advantages of some methods employed through this chapter's experiments, along with potential pitfalls to be kept in mind.


\subsection{Random baseline comparison}

Let us get an idea of the time it would have taken to annotate the sperm whale click database via random sampling for instance. Sperm whales were confirmed on 6\% of the files from Bombyx (see section \ref{chap:cacha_presence}). If we consider 30 seconds to manually check a file (between 1 and 5 minutes long), to yield the 2,300 positive labels collected here (they are each on separate files), one would need 320 hours. It took approximately 40 hours in total to collect this database with the annotation approach described in section \ref{chap:bombyx_annot} (custom \ac{UI} to visualise \ac{TDOA} tracks).


\subsection{Active learning}

Active learning has proven to be very efficient in iteratively increasing database sizes. It can be started as soon as few dozen annotations are at hand. Indeed, in that case, rather than spending time in tuning pre-detection mechanisms to collect more samples, deep learning models help to collect occurrences of the target signal as well as disruptors (e.g. boats, signals from other species). Moreover, it is worth the efforts of developing the training procedures since they will be used subsequently, as opposed to the pre-detection algorithms which are rather a one time usage.

Nonetheless, there is a danger that comes along with active learning: the progressive specialisation of the model to detect only one type of signal. Indeed, if the initial annotations omit some type of signals from a target species, it is likely that the model will never learn to detect them. This especially comes from the fact that we often only correct the positive predictions of the model, sorting out false positives (negative predictions usually come in much larger numbers, making it fastidious to find false negatives). To mitigate this effect, one can manually browse recordings around detections and annotate full sequences, or look for false negatives in low confidence negative predictions.


\subsection{Thumbnail picking}

Thumbnail picking allows to quickly validate detections or clusters to collect annotations. The only condition is to find a visualisation that fits a small sized image while allowing to make a decision on sample classes (small spectrograms work well for most stationary signals as seen in Fig.~\ref{fig:png_annot}). It is versatile and easily shareable to experts: the only prerequisite is to have a graphical file manager. It is fast and user friendly: you just need to click on files to select them and move them to a separate folder (cut and paste). Additionally, seeing multiple samples at once strongly helps the eye in discriminating singularities.

This last advantage can also be dangerous in the annotation process. Indeed, when sorting large folders to try and keep only one call type, one might always see similar calls at a time on the screen, but through scrolling, pitch contours might shift progressively. When this occurs, the annotator might feel like all the calls in the folder are similar, when in fact, the ones at the beginning are very different from the ones at the end. To mitigate this, indexes should be randomly permuted, since this progressive shift in call contour is likely to occur if files are sorted time wise, but very unlikely otherwise.


\subsection{`Generic' spectral features extraction}

Section \ref{chap:HB_annot} proposes a procedure suited to explore large banks of recordings by grouping events with similar content in terms of spectro-temporal energy distribution. It allowed to collect a first database of humpback whale calls. The success of such approach relies on several assumptions:
\begin{itemize} \setlength{\itemsep}{1pt}
    \item A minimal knowledge of the target signals is needed to configure the algorithm (e.g. frequency range, chunk length),
    \item Events are grouped independently of the temporal distribution of the energy in the spectrogram (e.g. upwards and downwards chirps will yield the same features). This is suited to discriminate between events of different temporal support, but would not work to discriminate between pitch patterns,
    \item For the projection and the clustering to reveal a group of events, a sufficient number of instances are needed. This is the most probable explanation of why we failed to retrieve dolphin whistles and click trains using this method (they were found to be way more scarce than humpback whale calls).
\end{itemize}