\chapter{State of the art}
\minitoc
\section{Neural networks for computer vision}

The following section introduces the main techniques involved in building and training \ac{CNN}s. Methods will be described in their most prevalent context in the literature (computer vision) before diving into \ac{PAM} use cases in the next chapter. 

If computer vision techniques are used to tackle acoustic tasks, it is mainly because we can represent sound with images called spectrograms. They reveal time / frequency content such that, for instance, vocalizations appear as patterns with identifiable shapes\footnote{In a way, our hearing system itself processes sound via a frequency decomposition with the cochlea}. We will therefore first go through the state of the art in image pattern recognition before applying similar methods to our acoustic tasks. This is obviously not an exhaustive review of deep neural networks, but rather an overview of the key elements to build detection and classification systems for images. %Subsequently, I will mention existing use cases of bioacoustics using \ac{ANN}s.


\subsection{Introduction to Artificial Neural Networks}

The idea of emulating brain neural systems computationally emerged in the mid 20th century \cite{fitch1944mcculloch}. It is however only recently that \ac{ANN}s have taken such an important part in applied mathematics and computer sciences, with the increased availability of data and computational power. The underlying approach to \acp{ANN} is to reproduce advanced processes emerging from the accumulation of simple operations, alike brains with neurons. Put mathematically, neurons would typically take the form of a simple linear transformation of inputs into an output signal (\(y = ax + b \)). With their combination into large networks emerges the capacity of modelling high level functions such as classifying cat and dog images. Building a handcrafted algorithm to address this task robustly would be nearly impossible, but \acp{ANN} easily find their own way through it.% Most relevant to this thesis are computer acoustics tasks, namely sound event detection and classification.

An \ac{ANN} is defined by a network architecture (interconnections of neurons) and its neurons' weights (what linear transformations are performed, namely \(a\) and \(b\)). %From this simple concept, the spectrum of functions they can model is immense. The most widespread are probably computer vision tasks, such as image classification (telling cats and dogs apart for instance). 
We first design an architecture before learning the optimum weights for our task, typically with supervised learning. Supervised learning works by feeding the model examples with their associated labels. For our cats and dogs tasks, this would mean giving the model images of each class, asking it to predict the associated label, namely `cat' or `dog'. Feeding lots of example images to the network, and iteratively updating its neurons' weights to optimize the performance at the task (e.g. classification accuracy) will simply do the trick. 

Under the hood, the network learns a projection of the input images (often called embedding) from the pixel space to a new abstract one. Put simply, the more neurons in a network, the more complex the resulting projection can be\footnote{Neurons are put in a stack of layers, thus the appellation `deep learning'}. Training thus becomes learning the optimum embedding space to solve a given task.

There are two main limitations here, the first being the necessary computational power. Training a large \ac{ANN} demands thousands of iterations, each of which consists in an update of millions of neurons. This is in part why we had to wait for the development of parallel computation with \acp{GPU} to see the democratisation of \acp{ANN}. The second limitation, this time a human effort cost, is the necessary training data. To learn a robust solution, training typically demands thousands of examples for each class, with their associated label (often manually annotated) for the computation of the accuracy that will be optimized.

\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{fig/cats and dogs.pdf}
    \caption{Illustration of the concepts of underfitting and overfitting, for the cats and dogs classification task. Lines denote discrimination boundaries, in a two-dimensional abstract embedding space.}
    \label{fig:catsanddogs}
\end{figure}

This leads us to the major challenge of training \ac{ANN}s : robustness, or generalization. It is faced by any modelling technique in every field, but \ac{ANN}s probably suffer from it the most. Indeed, optimizing a performance metric on a limited amount of examples might bring the curse of overfitting : when the model finds a solution that works for its given training data, but not the generic solution that we desire (see Fig.~\ref{fig:catsanddogs}). To give an example, coming back to the cats and dogs task, if all the cats we show the \ac{ANN} are white and all dogs are black, it might just discriminate on average pixel colors. This will lead to great performances on the training data, but will properly fail as soon as we try our \ac{ANN} on a black cat image. As we will see throughout this thesis, most of the struggle in training \ac{ANN}s comes down to enforcing generic solutions with limited training data.

%\section{Convolutional Neural Networks}
%In the following section, we will dive deeper into the category of \acp{ANN} that was used through this thesis : \acp{CNN}. I will start by describing its building blocks (layers of neurons), then going through the typical architectures they are put together in, and ending with how to iteratively optimize its neurons' weights while avoiding overfitting. I will use the original use case of \acp{CNN} for demonstration here (image classification), before joining in on this thesis' tasks in the next section.


\subsection{Layers}

\subsubsection{Convolution}

Convolution is a mathematical operation that describes the integral of the point-wise product of two functions, with a varying shift on the input variable. It is usually noted with the asterisk symbol (see Eq. \ref{eq:conv}, given a kernel $f$ of size $M$ and a function $g$).

\begin{equation}\label{eq:conv}
    (f*g)[n] = \sum_{m = 0}^{M} f[m] \times g[n-m]
\end{equation}

Typically, in image processing, we will use this operator to slide a filter (or kernel) over a larger image. The output of the convolution will be maximal where the filter matches most the image, or in other words where there is the strongest correlation. In 1995, \citet{lecun1995convolutional} introduced the concept of using convolution operators in neural networks; \acp{CNN} were born. 

Before that, pixels where given independently to input neurons. The input image size was thus fixed for a given network architecture, and a displacement of patterns within an image would mean a totally different response of the network. With \acp{CNN}, the network's neurons take the form of kernels (or filters), which are convolved onto input images. Like so, patterns are searched all over the image, independently of their placement.

This behaviour is called spatial invariance, and is crucial for pattern recognition in images (looking for a cat within an picture or a vocalization within a spectrogram for instance, independently of their placement). This characteristic led \acp{CNN} to become unavoidable in the field\footnote{Let aside the recent rise of transformers for computer vision \cite{parmar2018image}.}.

In terms of mathematical definitions, a traditional \ac{ANN} layer is described as $\mathbf{y = Ax + b}$ with $\mathbf{x}$ an input vector (output from the preceding layer), and y an output vector (input for the following layer). The weights $\mathbf{A}$ and $\mathbf{b}$ are thus matrices defined in $\mathbb{R}^{out\times in}$ and $\mathbb{R}^{out}$ respectively, with $in$ and $out$ being the number of neurons in the preceding and current layers respectively.

As for \acp{CNN}, a layer is no longer composed of a stack of neurons, but rather a stack of kernels. The behavior of a kernel of width $w_k$ and height $h_k$ is formulated by Eq. \ref{eq:kernel}, given an input of width $w$, height $h$, and depth $d$.

\begin{equation}\label{eq:kernel}
\mathbf{Y = A*X}+ b, \quad \mathbf{X} \in \mathbb{R}^{h \times w \times d}, \mathbf{A} \in \mathbb{R}^{h_k \times w_k \times d}, b \in \mathbb{R}
\end{equation}

The convolution integral is done over the 3 dimensions, but the shift will occur on the width and height dimensions only, making $\mathbf{Y} \in \mathbb{R}^{h \times w}$. The outputs of each kernel of the layer will eventually be stacked to form the depth dimension for the input of the next layer\footnote{The color dimension of input images are also put as depth dimension} (see Fig.~\ref{fig:convolution}).

\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{fig/conv_layer.pdf}
    \caption{Convolution layer. Blue denotes a slice of the image, a kernel, and the resulting point in the output image (the sum of the point-wise product between the two). The number of kernels will define the depth of the output cuboid.}
    \label{fig:convolution}
\end{figure}

A \ac{CNN} layer is thus defined by the number of input features it processes, its number of kernels, and their width and height. The number of trainable parameters in a layer is thus $d_{in} \times w_k \times h_k \times d_{out} + d_{out}$.


\subsubsection{Depth-wise separable convolution}\label{chap:sota_dw}

As presented in the previous section, convolution kernels are cuboids, with a depth that fits the depth of the input. The filters are thus designed to find patterns that are interconnected depth-wise. However, often, we might want patterns to be filtered independently through the input image depth, for a subsequent depth-wise combination. This is the idea introduced by depth-wise separable convolutions, first used in the context of a \ac{CNN} by \citet{chollet2017xception}.

In this new type of convolution layer, we dissociate the spatial filtering and the depth-wise combination in two stages, as opposed to regular convolutions that process it all at once. A kernel remains cubic, but the convolutions are separated depth-wise, thus yielding a cuboid, when a regular convolution kernel yields a flat image. The combination of the features then happens with the point-wise stage, similar to a convolution with a kernel of width and height 1. This stage can be repeated to obtain an output depth (see Fig.~\ref{fig:depthwiseConv}).

For comparison with the regular convolutions, the number of trainable parameters in a depth-wise separable convolution layer is $d_{in} \times ( w_k \times h_k + 1 + 2\times d_{out})$.% Furthermore, the number of multiplications needed to go through one layer becomes $ depth_{input} \times  n \times ( width_{kernel} \times height_{kernel} + depth_{output})$.

\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{fig/dw_conv_these.pdf}
    \caption{Depth-wise separable convolution. In the depth-wise stage, each depth bin is convolved with its own kernel independently. For the point-wise stage, the depth dimension is combined point by point by various vector kernels, each of which will result in a depth bin in the output.}
    \label{fig:depthwiseConv}
\end{figure}

Having less parameters involved in a network means less computational complexity for inference and for weight updates. Moreover, this type of convolution has shown improved generalisation performances for computer vision tasks \cite{chollet2017xception}. Indeed, limiting feature inter-dependence could limit potential overfitting, alike the dropout \cite{srivastava2014dropout} technique introduced later on.


\subsubsection{Pooling}

As previously mentioned, convolution enables spatial invariance. However, it doesn't treat the scale problem. Indeed, some patterns might have to be detected independently of their scale in input images. Moreover, detecting large patterns would require large kernels, which are expensive in computation and memory. For this purpose, pooling layers enable a progressive decrease in image resolution (on the width and height dimensions), so that deeper layers can have a larger scale view without requiring larger kernels.

%The preferred type of pooling layer is often max pooling, but it can come with any mathematical reduction operation. The end goal is the sub-sampling of the image, or reducing the resolution, while conserving the information that matters. A kernel will be slided over the input, with a stride superior to 1. The amount of stride for each dimension will imply the factor of decrease in resolution.

Often, we want to simply denote if features (depth bin) were activated in a given area, with a lower resolution. Max-pooling layers are well suited for this, simply keeping the maximum value in a window with a $stride>1$. Typically, max-pooling layers are placed after 2 or 3 convolution layers.% Also, they can come practical when we need to reduce down the spatial dimension to 1, as a last layer to the network for instance.


\subsubsection{Non-linearity layers}

Even if they are spatialized, convolution layers remain a simple linear transformation of the input, and one could argue that accumulating linear transformations successively is equivalent to a single linear transformation (see Eq.~\ref{eq:twolinear}).

\begin{equation}\label{eq:twolinear}
    a_2(a_1x + b_1) + b_2 = (a_2a_1)x + (a_2b_1 + b_2)
\end{equation}

Therefore, building deep networks by accumulating layers of neurons would not add to the complexity the network is able to model. In order to model non-linear functions up to a great complexity, non-linearity layers in-between linear layers are thus needed. Common non-linearity layers are \ac{ReLU}, leaky ReLU, TanH, among others.

Moreover, functions such as \ac{ReLU} allow to insert zeros in numerous dimensions of vectors. This serves the stabilisation of gradients during the optimization and has an effect of dimensionallity reduction.


\subsection{Architectures}

\subsubsection{Projection using \acp{CNN}}

Before \acp{ANN}, non linear \acp{SVM} \cite{aizerman1964theoretical} were used for a similar purpose : learning the optimum projection of data points to make them linearly separable. Only their approach to optimization differ. In our case study of \acp{CNN}, we typically want to project an image from the pixel space to a lower dimensional space that reflects semantic content. We often refer to \acp{CNN} as encoders for this projection property.

There are two complementary approaches to dimensionality reduction, in \acp{CNN} : pooling, and depth decrease. As seen previously, pooling reduces spatial dimensions to a factor of its stride. Thus, with three 3x3 pooling layers (usually intertwined with convolution layers), we break down a 128x128 image into a 5x5 image.

As for the depth decrease, in network architecture design, we usually increase the depth during the first layers (feature extraction part), and then reduce it down to the desired output depth (projection part). The projection part is often operated by fully connected layers (traditional \ac{ANN} layer) after flattening the image (compression of the width, height, and depth into a single dimension, see Fig.\ref{fig:vgg}).

\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{fig/vgg16.png}
    \caption{VGG16 architecture \cite{simonyan2014very}. Dimensions at each layer are given in this order : $height \times width \times depth$ (image taken from \citet{ferguson2017automatic}).}
    \label{fig:vgg}
\end{figure}

\subsubsection{Interpretation of the model's output}

The depth of the desired output, for classifiers, is defined by the number of possible classes for our task. Indeed, each output feature will describe the confidence of the model on the presence of one class in the input. We will thus train our model to, given an input and its associated class(es), maximize the confidence value of the `present' class(es), while minimizing those of the `absent' class(es). For the case of binary classification, networks can have either one or two output feature(s)\footnote{Single output encoders can be seen as detection systems, as we will see through several use cases in this thesis.}.

\subsubsection{Standard architectures}
Numerous standardised architectures are used by the deep learning community as, in most cases, starting the design of a new architecture from scratch seems unnecessary and counterproductive (as long as the task at hand is relatively similar to the one of the standard architecture). Through this thesis, experiments will make use of three architectures coming from the ImageNet computer vision benchmark \cite{deng2009imagenet} : VGG16 \cite{simonyan2014very} and ResNet18/50 \cite{he2016deep}. 

\begin{itemize}
    \item The VGG architecture is presented in Fig.~\ref{fig:vgg}, and is a classic convolutional encoder tailed by fully connected layers. 
    \item The ResNet18 and ResNet50 architectures are composed of residual blocks, which introduce `skip-connections' (the output of a block is the sum of its processed input and the original input). Their associated number denotes the number of layers that composes them.
\end{itemize}

These three architectures were chosen as they are (or have been) the baseline in image classification tasks, therefore considered standard \ac{CNN} architectures, even for bioacoustic tasks (see section \ref{chap:PAM_use_cases}).

\subsubsection{Encoders as part of more complex systems}\label{chap:AE}
As seen in the previous section, encoders can serve classification tasks, but they can also take part in bigger systems such as \acp{AE}. \acp{AE} serve tasks of dimensionality reduction, operated with an encoder similar to classifier architectures. To enforce the conservation of information, the encoder is followed by a decoder, that reconstructs the input image from the low dimensional space (called bottleneck). The encoder and decoder combination (called \ac{AE}) is trained to compress and reconstruct the input most faithfully, despite the low dimensional bottleneck.

The compression that \acp{AE} offers enables a removal of random or unstructured information (denoising), and a lower dimensional space for a better clustering\footnote{Clustering relies on sample distance estimations which are unreliable in the pixel space and suffer the curse of dimensionality}.


%not relevant here : still classif encoders 
%Using multiple parallel encoders (Teacher student, meta pseudo label, siamese)


%\subsubsection{Other architectures}
%There are
%RNN / LSTM : suited for temporal dependencies, used in language modelling

%Regression for localisation estimate


\subsection{Performance optimization}

As previously mentioned, \ac{ANN} parameters are iteratively updated to optimize performances at a given task. This optimization usually takes form as the minimization of some error function, or loss. We will follow the loss curve downwards w.r.t. each parameter by computing their gradient, and so until convergence.

\subsubsection{Classification and detection losses}

Because it will be needed for gradient computation, the chosen loss to optimize needs to be differentiable. For our classification tasks, the accuracy is therefore not suitable since it relies on the $argmax$ of the output vector. We will rather choose the \ac{CE} instead, and will keep the accuracy for model validation for its interpretability.

The definition of the \ac{CE} classification loss $H$ is given in Eq.~\ref{eq:CE}, with $y$ the one-hot encoded label, $\hat{y}$ the vector of predicted probabilities for each class, and $C$ the set of possible classes.

\begin{equation}\label{eq:CE}
    H(\mathbf{y}, \mathbf{\hat{y}}) = -\sum_{c\in C}\mathbf{y}_c log(\mathbf{\hat{y}}_c)
\end{equation}

To get normalized predictions of the model resembling a probability distribution, we use the SoftMax function described in Eq.~\ref{eq:softmax}, given the unnormalized model output $\mathbf{z}$ (also called logits).

\begin{equation}\label{eq:softmax}
    SoftMax(\mathbf{z})_i = \frac{e^{\mathbf{z}_i}}{\sum_{k}e^{\mathbf{z}_k}}
\end{equation}

This is appropriate for the multi-class classification tasks, when a higher confidence for an instance implies lower probabilities for others. When solving multi-label classification tasks however, two objects can occur simultaneously, making the SoftMax assumption not relevant. The Sigmoid function is then rather used to normalize logits to probability distributions (Eq.~\ref{eq:sigmoid}), and the sum of the independent \acp{BCE} as a loss.

\begin{equation}\label{eq:sigmoid}
    Sigmoid(\mathbf{z}) = \frac{1}{1+e^{-\mathbf{z}}}
\end{equation}

The \ac{BCE} is simply a special case of the \ac{CE}, with $C=2$. However, we can use single valued labels $y$ and $\hat{y}$ for its computation (Eq.~\ref{eq:BCE}).

\begin{equation}\label{eq:BCE}
    BCE(y, \hat{y}) = ylog(\hat{y}) + (1-y)log(1-\hat{y})
\end{equation}

\begin{figure}[!htb]
   \begin{minipage}{0.5\textwidth}
     \centering
     \includegraphics[width=\linewidth]{fig/sigmoid.pdf}
     \caption{Sigmoid function (Eq. \ref{eq:sigmoid}).}
   \end{minipage}\hfill
   \begin{minipage}{0.5\textwidth}
     \centering
     \includegraphics[width=\linewidth]{fig/BCE.pdf}
     \caption{\ac{BCE} loss (Eq. \ref{eq:BCE}).}
   \end{minipage}\hfill
\end{figure}

Through this thesis, the binary classification will be used as a proxy to solve detection tasks (one class being the target event to detect, and the other anything else). When running a classifier model post training, the predicted class of will be $argmax(z)$. For detectors, a detection is triggered when the scalar output is above a given threshold.


\subsubsection{Representation learning losses}\label{chap:ssl}

The losses previously mentioned are suited when a sufficient amount of labels are available for supervised learning. When few or no labels are available, frameworks were designed to learn semantically relevant embedding spaces, used subsequently by clustering algorithms or in supervised fine tuning. We call this process deep representation learning or metric learning. Since this learning paradigm does not rely on labels for optimization, it is referred to as \ac{SSL}.

\paragraph{Triplet loss and contrastive learning}\label{chap:triplet}
Contrastive learning is a branch of \ac{SSL} algorithms, where we enforce the models' projection to ignore transformations applied to the input (transformations that do not imply a semantical change to the data). For that purpose, we will minimize the distance between the projection of a sample and that of its transformation w.r.t. the projection of other samples (see Fig. \ref{fig:contrastive}). The mathematical formulation of this objective is termed as triplet loss since it uses the projection of three samples : an anchor (the original sample), a positive (the transformation of the anchor), and a negative (another unrelated sample). Several metrics have been used in the literature to measure distances between embeddings :

\begin{itemize}\setlength{\itemsep}{1pt}
    \item The cosine similarity (SimCLR \cite{chen2020simple})
    \item The cross-entropy (\acs{UDA} \cite{xie2019unsupervised}, fixMatch \cite{sohn2020fixmatch})
    \item The cross-correlation (Barlow \cite{zbontar2021barlow}) 
    \item The mutual information (\ac{IIC} \cite{ji2018invariant})
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{fig/contrastive learning.pdf}
    \caption{Illustration of the contrastive learning approach. The anchor and the negative are randomly sampled from the database, whereas the positive is a hand crafted transformation of the anchor. The distance metric to be minimized/maximized varies among implementations.}
    \label{fig:contrastive}
\end{figure}

These contrastive losses can also be combined with a regular classification loss for a semi-supervised framework, as seen in fixMatch \cite{sohn2020fixmatch} for instance. They then can be considered as a form of training regularization (see section \ref{chap:regularization}).

\paragraph{Triplet loss and Siamese neural networks}
In a similar fashion than with contrastive learning, the triplet loss can be used in a supervised context. In this case, the positive of the triplet is a sample drawn from the same class as the anchor, and the negative is a sample from another class. We call this approach Siamese networks \cite{bromley1993signature, koch2015siamese}. Despite its use of labelled samples alike regular supervised classification training, this method focuses on learning an embedding space to measure samples' similarity, rather than an embedding space to discriminate among classes.

\paragraph{Reconstruction loss}\label{chap:reconstruction}
In other \ac{SSL} frameworks such as \acp{AE}, we will use a reconstruction loss, that reflects the fidelity of the reconstructed sample w.r.t. the original input. This can simply take the form of a \ac{MSE} between the input and the reconstructed image (pixel loss). There are also more advanced approaches such as the perceptual loss which uses the \ac{MSE} in the latent space of a third party encoder to have comparison at a higher level than pixel wise \cite{johnson2016perceptual}.

\subsubsection{Optimizing the loss}

Depending on our task and label availability, we now have a continuous metric of error to be minimized. A straightforward way of finding some function's minima is to follow the slope downwards. This is formulated by the \ac{SGD} algorithm, as expressed in Eq.~\ref{eq:sgd}, with the update of the parameters $\theta$ depending on the gradient of the loss $L$. The amplitude of the update is defined by the learning rate $\alpha$, defined between 0 and 1.

\begin{equation}\label{eq:sgd}
    \mathbf{\theta^{(i+1)}} = \mathbf{\theta^{(i)}} - \alpha \frac{\nabla L}{\nabla \mathbf{\theta}}
\end{equation}

The definition of the learning rate is crucial to a qualitative convergence of the model's parameters. Indeed, a two small learning rate might result in getting stuck in local minimas, whereas with a too large one the actual minima might be skipped. No generic learning rate is good for every task, so it will be one of the hyper-parameters to be tuned (see section \ref{chap:HP_tuning}). 

To enhance convergence quality and speed, the community is now opting for learning rates that evolve through the course of the optimization. This evolution (termed learning rate scheduling) can be a simple exponential decay, a decay when the loss plateaus, or more advanced periodic schedules with warm restarts \cite{loshchilov2016sgdr}. No definite agreement has yet been made on the right schedule, and the answer might again be task specific.

Methods like \ac{SGD} to iteratively update the model's parameters depending on the loss gradient are called optimizers. \ac{SGD} has seen several upgrades since its original formulation, especially with gradient smoothing. The nesterov momentum \cite{sutskever2013importance} as well as the gradients' moments \cite{kingma2014adam} serve that purpose. Other techniques such as batch normalization also indirectly work towards gradient smoothing to ease the optimizer's work \cite{santurkar2018does}.


\subsubsection{Model validation}
Once our model has optimized the loss function until convergence, we usually want to measure its performance with interpretable metrics, and with new data.

\paragraph{Performance validation metrics}
For detection tasks, which most use cases of this thesis are, we will use the areas under the \ac{ROC} and \ac{PR} curves. They give average values of recall/fall-out and precision/recall respectively, for varying thresholds. Note that the area under the \ac{ROC} and \ac{PR} curves will be referred to as \ac{AUC} and \ac{mAP} respectively. Equations \ref{eq:AUC} and \ref{eq:mAP} formulate their computation with $rec$, $fo$, and $spec$ denoting recall, precision and fall-out respectively. $TP$, $P$, $PP$, $FP$, and $N$ denote numbers of true positives, positive ground truths, positive predictions, false positives, and negative ground truths respectively. Some are a function of a threshold noted $\lambda$, used to binarise continuous prediction values.

\begin{equation}
    rec(\lambda) = \frac{TP(\lambda)}{P}; \quad prec(\lambda) = \frac{TP(\lambda)}{PP(\lambda)}; \quad fal(\lambda) = \frac{FP(\lambda)}{N}
\end{equation}
\begin{equation}\label{eq:AUC}
    AUC = \int_{0}^{1} rec(\lambda) \; dfal(\lambda)
\end{equation}
\begin{equation}\label{eq:mAP}
    mAP = \int_{0}^{1} rec(\lambda) \; dprec(\lambda)
\end{equation}

These two metrics are similar, but differ on the measurement of false alarm rate : the \ac{mAP} normalises on the number positive predictions whereas the \ac{AUC} normalises on the number of negative samples. This difference has a significant impact especially with imbalanced datasets.

For multi-label classification tasks, we will average the independent detection performance of each class. As for multi-class classification, we will rather compute the accuracy as the rate of correct predictions. Averaging methods for the performance metric should be chosen to account for class imbalance or not (i.e. averaging the performance per class before averaging between classes or averaging performances per samples directly).

\paragraph{Validating with new data}

New data should be used for these performance measures. It is called the test set, as opposed to the training set which is used for the iterative loss optimization. When designing experiments, one must ensure that the test set is significantly different from the training set to relevantly measure generalisation. For instance, in sound event detection tasks, we might want to test our model on recording devices, environments, and emitters that have not been trained on. How well the model performs facing such domain shifts is the only reliable measure that should be taken into account. In the contrary, if a model has been trained and tested on similar data, a large performance drop should be expected as soon as the data changes.


\subsubsection{Hyper-parameter tuning}\label{chap:HP_tuning}

We mentioned the iterative optimization of the loss through the update of the model's weights, but other variables can be tuned to enhance performance. The model architecture and the optimizer have numerous settings that need to be fixed before training and have a huge impact on the training in both convergence speed and the found loss minima. We call them hyper-parameters. 

Each model training taking at least several minutes on a super computer, one cannot try all possible hyper-parameter combinations. Algorithms have been designed to explore the search space of hyper-parameters with the most efficiency. They combine several principles among which the early stopping of low performing models \cite{li2020system}, as well as muting high performing models for the next trials \cite{jaderberg2017population}.


\subsection{Training regularization} \label{chap:regularization}

Methods employed during training to reduce potential overfitting and enhance generalization are called regularization. In general these approaches come down to increasing the variability of the data both in the input and directly in the activations of the network.


\subsubsection{Data Augmentation}

Introducing variability to the input data is one of the most efficient ways of avoiding overfitting, especially with small datasets. The idea is to generate new data samples out of the existing ones, thus increasing the dataset size, without needing more annotation. To do so, we apply randomized transformations, realistic or not, with the only constraints that we must ensure not to change the ground truth label. For computer vision, RandAugment \cite{cubuk2020randaugment} has now been accepted as the standard augmentation policies, combining texture and shape transformations (see Fig.~\ref{fig:randAugm}). We will go through data augmentation for acoustic tasks in section \ref{accoustic_augm}.

\begin{figure}
    \centering
    \includegraphics[width=.75\linewidth]{fig/randaugmt.pdf}
    \caption{Using data augmentation enables new samples to be derived from original ones, while conserving the label. Transformations are randomly sampled among several texture and position alterations.}\label{fig:randAugm}
\end{figure}

Another branch of data augmentation worth mentioning is MixUp \cite{zhang2017mixup}, which combines two input samples and their labels, thus creating `in-between' data points. The combination takes form as a simple weighted sum of inputs and labels, which we will feed our model with like a regular sample. This simple concept of giving mixtures of 2 instances as training samples has shown to improve generalization in most computer vision tasks with standard architectures \cite{zhang2017mixup}.


\subsubsection{Within-network regularization}

By introducing perturbations and variability within the network, we can mitigate its dependency to highly specific events, and thus presumably increase its robustness. Dropout \cite{srivastava2014dropout} follows that incentive by randomly deactivating neurons or kernels (putting their activation to 0). The probability of discard is defined by the dropout hyper-parameter $p$, commonly set to 0.25.

A second common way to regularize the network while training is to enforce the model to rely on as few weights as possible. To do so, we introduce a new term in the loss : the $L_2$ norm of all parameters, weighted to control its impact. We call this method weight decay \cite{krogh1992simple}, and its weight introduces another hyper-parameter to the learning framework.


\subsubsection{Leveraging unlabelled samples}\label{chap:fixMatch}

As seen in section \ref{chap:triplet}, contrastive losses can be used to train encoders for resilience to data augmentation. Several algorithms have been published to incorporate this, often termed as consistency training. Tab. \ref{fig:perf_fixmatch} summarises their performances on \ac{SSL} datasets with varying proportions of labelled samples. The fixMatch algorithm \cite{sohn2020fixmatch} combines a supervised loss, pseudo labelling, and consistency training in one framework to achieve \ac{SOTA} performances for the tasks with the fewest labels.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/table_fixmatch.png}
    \caption{Error rates on CIFAR-10, CIFAR-100, SVHN and STL-10 on 5 different folds (taken from \citet{sohn2020fixmatch}).}
    \label{fig:perf_fixmatch}
\end{figure}